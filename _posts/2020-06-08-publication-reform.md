---
layout: post
title: Science isn't broken, but journals are
date: 2020-06-08
tag: 
   - Metascience
description: A joint solution is emerging for disparate problems of access, quality control and replicability in scientific publishing.
image: /img/0185-loupy-ellis-tweets.png
socialimage: http://freerangestats.info/img/0185-loupy-ellis-tweets.png
category: Other
---

<i>This post jointly written by Peter Ellis and Kay Fisher.</i>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I can understand this position that editor filtering and peer review are hard but it begs the real question of what value are journals adding? Why not publish just on the pre-print servers and move all review to PubPeer or similar. Skip Elsevier et al completely.</p>&mdash; Peter Ellis (@ellis2013nz) <a href="https://twitter.com/ellis2013nz/status/1269400072187338752?ref_src=twsrc%5Etfw">June 6, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/reviewer2?src=hash&amp;ref_src=twsrc%5Etfw">#reviewer2</a> : &quot;I don&#39;t have to read very much of this manuscript to realize there is nothing meaningful that can be learned from it&quot; ffs if you&#39;re not going to read it, don&#39;t review it. ðŸ˜ </p>&mdash; Ben Bond-Lamberty (@BenBondLamberty) <a href="https://twitter.com/BenBondLamberty/status/1268641821493620736?ref_src=twsrc%5Etfw">June 4, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

The scientific publishing process has a range of well known problems, the symptoms of which include:

1. Access to published science is restricted - journal subscriptions are expensive items even for university libraries, and basically not available to others. See for example the critique from [The Cost of Knowledge](http://thecostofknowledge.com/)
2. Researchers [resent petty gatekeeping by anonymous reviewers](https://twitter.com/search?q=%23reviewer2&src=typed_query), which is sometimes in an offensive spirit and sometimes requires changes the authors are convinced subtract value (for example, insisting on an outmoded statistical technique because it is familiar to the reviewer; or insisting on referencing the reviewer's own favourite publications). 
3. Academics find peer review difficult and unrewarding work. Some of them resent giving free labour to profit-seeking firms (see point 1). Editors complain that it can be hard to get enough of the right reviewers.
4. The gatekeeping isn't effective in assuring readers of research quality. If something as transparently bad as the Surgisphere fraud could pass the editorial filter and peer review processes of the Lancet and the New England Journal of Medicine and it is expected that post-publication peer review is required to pick up the flaws, then it is genuinely hard to understand what the imprimatur of "published in a peer review journal" adds. In fact, the authority granted by passing this (apparently not to high) bar makes post-publication peer review harder
5. On the other hand, readers from outside of the scientific establishment are now exposed more than previously to "pre-print" science. Many people fear that preprints are given too much exposure, given they have not been subjected to peer review. The media and Twitter cycles and the nature of public debate are powerful forces for making this worse, for example by creating amplifiers for misinterpretations or overinterpretations based on single studies.
6. At the same as all this, we have the [replication crisis](https://en.wikipedia.org/wiki/Replication_crisis), the growing realisation that much published science is based on shoddy methods, statistical or otherwise. While great advances have been made in combating this (pre-registration, open code and data, improved statistics, improved meta-studies), problems remain with incentives and failures from all the methods above.  

Ionnadis as one of the leaders in exposing the problems of #6 [nb link to his article], yet an example of problems with #5. Surgisphere case illustrates the problems with #3 and #4.

All this may seem bad, but the joint solution to all these problems is emerging fast. Much of it in fact is already in place. Here is what I see as the elements of the solution, in rough order of how it will emerge.

1. Make the preprint servers the default for publication.
2. Strengthen existing post-publication peer review as executed on [PubPeer] - the question-based, problem-seeking type.
3. Complement this with a new positive form of post-publication peer review - badges or certificates indicating a stamp of approval from an appropriately certified reviewer, covering off just one dimension of quality. Articles that accrue gold-standard badges on all dimensions can be treated as gold-standard research, with much greater confidence than we can now something.
4. Gradually abandon "pre-publication" peer review. There really is no such thing as pre-publication any more, with researchers announcing findings by press release, blogs and preprints.
5. Let the journals wither away, or re-invent themselves as open-access journals that are essentially collections of articles in the manner of "Peeriodicals" promoted by PubPeer. 

As mentioned, this solution is already evolving spontaneously, with the basics of items 1 and 2 in place and growing in strength. The key features have been: 

- the rapid rise of the pre-print servers 
- improved post-publication peer review through sites like PubPeer
- growing expectations of open data and (at least) open code, with acting on those expectations made easy by sites like GitHub.

Peer-reviewed Open Access journals are also an important part of the movement towards a solution. However, I am arguing that the whole concept of a journal and pre-publication peer review are out-moded. I think these can be seen as very much an interim step rather than likely to be part of an enduring solution (of course, existing open access journals, and maybe even closed journals, are likely to evolve or re-invent themselves into something useful along the lines of part #5 of my solution)

The only new part of our proposed solution is item #3, badges or certificates. We believe these are a necessary complement to the sort of post-publication peer review that evolves spontaneously and is likely to be of the critical, problem-finding, question-asking type. That sort of review is essential but not enough for a reader to know quickly and clearly the extent to which a given article is of sufficient quality to be relied upon, nor how well and in which aspects. Both time-poor and/or expertise-limited readers (both researchers and the general public) will continue to need efficient shortcut indicators of research quality and also reduce unnecessary repeats of quality assessments. We think it is unreasonable to expect researchers, let alone the public, to be sufficiently expert in all pertinent dimensions of quality of every study. If nothing else, something will be needed to replace the job that the relative prestige of peer-reviewed journals currently provides as an indicator of research quality (even if very imperfectly). 

This is an opportunity to do that job better. Badges and certificates, as indicators of quality would allow each specific dimension of quality to be reviewed by experts with relevant expertise. This means that peer review can be done by multiple experts focusing only on the areas in which they have relevant and sufficient expertise. This would encourage more cross-discipline and specialist expertise to be drawn upon for assessing specific areas, and opens up the possibility for reviews on specific dimensions of quality to be accumulated over time in an open peer review environment. While holistic reviews may still be part of this, the more badges or certificates on specific dimensions that a study has achieved, the more it indicates the research is likely to be robust. It also allows research to be recognised for its strengths, even though it may have weaknesses or areas where caution may be needed. Which probably describes the vast majority of research. But of course this will mean we need to identify what is sufficient expertise and how reviewers would qualify (more on that later).

Of course the dimensions of quality and how they are bundled would need to be jointly developed by the scientific community and perhaps adjusted for different types research. But we expect they would cover the following:

<ol type="a">
<li>publication describes and takes into account previous literature in the field </li>
<li>publication describes and takes into account relevant literature in other fields (likely different expertise required to above)</li>
<li>study rationale is well reasoned and compelling, and makes a new contribution to the field, either with new findings or replicating or failing to replicate previous findings</li>
<li>methods are fully described and any software (ie code) developed for the research works as described</li>
<li>statistical and/or qualitative methods, and their execution, are sound</li>
<li>data provenance is verified</li>
<li>results can be reproduced with the original data (data transparency necessary for both quantitative and qualitative research)</li>
<li>writing is clear and concise</li>
<li>data visualisations or tabulations are relevant, sufficient and follow agreed good practice</li>
<li>conclusions follow from the analysis</li>
<li>results have been replicated with new data in subsequent studies (this would accumulate over time).</li>
</ol>

And so on. As soon we set out a list like this it is obvious that different experts are needed for most of these dimensions, and how impossible it is for existing pre-publication review to check off all of these in a timely fashion with only two reviewers. In fact, it seems that existing pre-publication peer review is limited to only a few of these dimensions in any one instance, with the issues covered depending to a significant degree on the luck of the draw of the expertise and interest of the particular reviewer.

The Imperial College microsimulation in C as an example of a massive effort just to check "any software developed for the research works as described".

The need to allocate resources somehow to the peer review, particularly the positive badging. People will spontaneously do the problem-seeking type.


