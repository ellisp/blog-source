---
layout: post
title: Test-positivity rates and difficulties of estimating effective reproduction number 
date: 2020-05-03
tag: 
   - ModellingStrategy
   - Health
   - R
description: I look at several different ways of accounting for the information given us by high positive testing rates for COVID-19 and look at the impact on estimates of effective reproduction number at a point in time
image: /img/0178-smoothed-test-rates.svg
socialimage: http://freerangestats.info/img/0178-smoothed-test-rates.svg
category: R
---

One major inference problem presented by the COVID-19 pandemic is estimating population prevalence, its growth and [effective reproduction number](https://www.healthknowledge.org.uk/public-health-textbook/research-methods/1a-epidemiology/epidemic-theory), given a lack of adequate random samples from populations, and the very partial coverage of non-random testing (ie of people with symptoms) in many areas. In the US in particular, a very high percentage of people tested return a positive result, [strongly suggesting that a large but unknown number of untested people also have the disease](https://www.theatlantic.com/technology/archive/2020/04/us-coronavirus-outbreak-out-control-test-positivity-rate/610132/).

In late March and early April of this year, fully 50% of tests in New Jersey and New York were positive:

<object type="image/svg+xml" data='/img/0178-smoothed-test-rates.svg' width='100%'><img src='/img/0178-smoothed-test-rates.png' width='100%'></object>

*All R code is at the bottom of the post today. Any sources that aren't clear from the text and graphics are explicit in the code.*

The statistical conundrum here can be thought of in a simple numeric example. Suppose we have two locations, A and B. Both have 10 confirmed cases, but for A these are based on 20 tests (test-positivity of 50%) and in B they are based on 1,000 tests (1%, approximately the rate [here in Australia](https://www.health.gov.au/news/health-alerts/novel-coronavirus-2019-ncov-health-alert/coronavirus-covid-19-current-situation-and-case-numbers) at the time of writing). We can imagine several unrealistic extreme approaches in how we treat the inference from these confirmed case numbers to the number of people in the population with the disease:

1. We can accept them on face value and say each location has 10 cases.
2. We can multiply them both by some common factor ("10" is commonly thrown around) to represent undiagnosed cases (whether these are unsymptomatic, not-yet-symptomatic, or symptomatic and can't get a test) and say each location has 100 cases.
3. We can treat the tests as a sample from the population and multiply the cases in A by 50 to bring its cases up to "what they would be if they'd done as many tests as B", giving us 500 cases in A and 10 in B. In situations with more diverse numbers, this can be generalised by multiplying confirmed cases by the test-positivity rate and also by some arbitrary constant that is at least as large as the inverse of the lowest test-positivity of any location.

I should stress at this point that as far as I am aware, no serious commentator proposes any of these methods to be satisfactory, although method 2 is the choice of "nothing better" I have observed most commonly.

Method 3 has more intuitive appeal to me than either of the others. For example, it seems extraordinary to suggest that when New York had a test-positivity rate of 50% that there weren't many, many more people who would have been confirmed if tests were as widely available per sick person as is the case in Australia today. However, method 3 has problems too. In particular, there is an implicit assumption that increasing the testing 'sample' would see a steady test-positivity rate; whereas surely as more people were tested, the testing would be extended eventually to marginal cases and the rate of positives would start to decline.

All of this is a consequence of the difficulties of inference to an unseen population without a random sample, or at least a sample generated by a known process. Because the selection of the sample in this case is inextricably tied up with the variable we want to measure, we would need a very very good model indeed of the testing process to be able to meaningfully draw inferences from confirmed cases to population prevalence.

For some purposes including forecasting so long as total cases are well below herd immunity levels (which they certainly are), growth rates are more important than the absolute level. If we had to choose between methods 1, 2 and 3 in converting our confirmed cases to population estimates, what is the impact on estimates of the effective reproduction number (R) at a point in time? The excellent [Oz COVID 19 Visualisations](https://cbdrh.github.io/ozcoviz/) site (a joint effort of researchers at four different universities in New South Wales and Victoria and built as an RMarkdown dashboard) estimates effective R for NSW with both methods 1 and 2, observing that it makes very little difference. But what about my proposed method 3, which would inflate case numbers at times of less testing, relative to times of plentiful testing?

We can actually generalise all three methods into a model

$$y = mxq^k\epsiclon $$

where

- `y` is the population prevalence
- `x` is the confirmed cases
- `m` is an unknown constant multiplier of `x`
- `q` is the test-positivity rate
- `k` is an unknown constant exponent of `q`, between zero and one.
- $\epsilon$ is a random error term of unknown distribution

When `k` is zero we get method 2 - just multiply the confirmed cases by a constant. When `k` is 1 we get method 3, ie assuming the number of confirmed cases would increase poportionately if tests were hypothetically increased. Any value of `k` between those extremes is a nice compromise between the two, conceding the commonsense notion that the higher the test-positivity, the more cases this suggests are out there, without going to the extreme of full-on method 3.

The problem of course is that we don't know what either `m` or `k` are, nor is there any obvious means of estimating them. If we had an independent estimate of population prevalence we could do this, but we don't yet have such a measure of any reliability, certainly not a time series by location. Random samples for testing from the population is the way to do this, but you need [large samples and tests with very low false-positive rates to do this reliably](https://www.statschat.org.nz/2020/04/25/why-new-york-is-different/) if the disease is rare. 

One alternative method is to take the number of deaths "later" as an indicator of the number of actual cases now. Deaths from COVID-19 are more reliably counted than cases, although there is still [almost certainly a significant undercount](https://www.newscientist.com/article/mg24632804-100-how-many-people-have-really-died-from-covid-19-so-far/). There is also the problem that we don't know the infection fatality rate to any degree of precision (although expert opinion, not without dissent, is converging on a range of of around 0.5% to 1%). Anyway, it turns out we can get a face-plausible estimate of actual cases in New York by multiplying deaths seven days later by 100.

The chart below shows estimates of total cases in New York based on my three methods enumerated above, a "generalized" case where I use `k = 0.5`, and the "deaths seven days later multiplied by 100" methods.

<object type="image/svg+xml" data='/img/0178-diff-methods.svg' width='100%'><img src='/img/0178-diff-methods.png' width='100%'></object>

... and here is a smoothed version:

<object type="image/svg+xml" data='/img/0178-diff-methods-smoothed.svg' width='100%'><img src='/img/0178-diff-methods-smoothed.png' width='100%'></object>


<object type="image/svg+xml" data='/img/0178-total-cases.svg' width='100%'><img src='/img/0178-total-cases.png' width='100%'></object>


{% highlight R lineanchors %}

{% endhighlight %}

<object type="image/svg+xml" data='/img/0178-r-methods.svg' width='100%'><img src='/img/0178-r-methods.png' width='100%'></object>

